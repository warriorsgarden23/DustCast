{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a48bbcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CSV file...\n",
      "Initial data shape: (1031520, 44)\n",
      "Columns: ['lon', 'lat', 'time', 'level', 'z_min', 'z_max', 'z_mean', 'r_min', 'r_max', 'r_mean', 't_min', 't_max', 't_mean', 'u_min', 'u_max', 'u_mean', 'v_min', 'v_max', 'v_mean', 'w_min', 'w_max', 'w_mean', 'vo_min', 'vo_max', 'vo_mean', 'DUCMASS_mean', 'DUFLUXU_min', 'DUFLUXU_max', 'DUFLUXU_mean', 'DUFLUXV_min', 'DUFLUXV_max', 'DUFLUXV_mean', 'DMI_EAST_HadISST1.1', 'DMI_HadISST1.1', 'Month', 'lag_1', 'ensemble_predictions', 'squared_error_ensemble', 'lr_predictions', 'knn_predictions', 'dt_predictions', 'rf_predictions', 'h3_res_3', 'h3_res_4']\n",
      "Unique levels before filtering: ['10' '50' '100' '200' '300' '500' '700' '850' '925' '1000']\n",
      "Data shape after filtering levels: (1031520, 44)\n",
      "Unique levels after filtering: ['10' '50' '100' '200' '300' '500' '700' '850' '925' '1000']\n",
      "Unique Months: ['2023-01' '2023-02' '2023-03' '2023-04' '2023-05' '2023-06' '2023-07'\n",
      " '2023-08' '2023-09' '2023-10' '2023-11' '2023-12']\n",
      "Grouping keys: ['lon', 'lat', 'Month']\n",
      "Columns to average: ['z_min', 'z_max', 'z_mean', 'r_min', 'r_max', 'r_mean', 't_min', 't_max', 't_mean', 'u_min', 'u_max', 'u_mean', 'v_min', 'v_max', 'v_mean', 'w_min', 'w_max', 'w_mean', 'vo_min', 'vo_max', 'vo_mean', 'DUCMASS_mean', 'DUFLUXU_min', 'DUFLUXU_max', 'DUFLUXU_mean', 'DUFLUXV_min', 'DUFLUXV_max', 'DUFLUXV_mean', 'lag_1', 'ensemble_predictions', 'squared_error_ensemble', 'lr_predictions', 'knn_predictions', 'dt_predictions', 'rf_predictions']\n",
      "Other columns to keep: ['time', 'level', 'DMI_EAST_HadISST1.1', 'DMI_HadISST1.1', 'h3_res_3', 'h3_res_4']\n",
      "Aggregation dictionary:\n",
      "{'z_min': 'mean', 'z_max': 'mean', 'z_mean': 'mean', 'r_min': 'mean', 'r_max': 'mean', 'r_mean': 'mean', 't_min': 'mean', 't_max': 'mean', 't_mean': 'mean', 'u_min': 'mean', 'u_max': 'mean', 'u_mean': 'mean', 'v_min': 'mean', 'v_max': 'mean', 'v_mean': 'mean', 'w_min': 'mean', 'w_max': 'mean', 'w_mean': 'mean', 'vo_min': 'mean', 'vo_max': 'mean', 'vo_mean': 'mean', 'DUCMASS_mean': 'mean', 'DUFLUXU_min': 'mean', 'DUFLUXU_max': 'mean', 'DUFLUXU_mean': 'mean', 'DUFLUXV_min': 'mean', 'DUFLUXV_max': 'mean', 'DUFLUXV_mean': 'mean', 'lag_1': 'mean', 'ensemble_predictions': 'mean', 'squared_error_ensemble': 'mean', 'lr_predictions': 'mean', 'knn_predictions': 'mean', 'dt_predictions': 'mean', 'rf_predictions': 'mean', 'time': 'first', 'level': 'first', 'DMI_EAST_HadISST1.1': 'first', 'DMI_HadISST1.1': 'first', 'h3_res_3': 'first', 'h3_res_4': 'first'}\n",
      "Grouping data by keys: ['lon', 'lat', 'Month']\n",
      "Grouped data shape: (103152, 44)\n",
      "Grouped data columns: ['lon', 'lat', 'Month', 'z_min', 'z_max', 'z_mean', 'r_min', 'r_max', 'r_mean', 't_min', 't_max', 't_mean', 'u_min', 'u_max', 'u_mean', 'v_min', 'v_max', 'v_mean', 'w_min', 'w_max', 'w_mean', 'vo_min', 'vo_max', 'vo_mean', 'DUCMASS_mean', 'DUFLUXU_min', 'DUFLUXU_max', 'DUFLUXU_mean', 'DUFLUXV_min', 'DUFLUXV_max', 'DUFLUXV_mean', 'lag_1', 'ensemble_predictions', 'squared_error_ensemble', 'lr_predictions', 'knn_predictions', 'dt_predictions', 'rf_predictions', 'time', 'level', 'DMI_EAST_HadISST1.1', 'DMI_HadISST1.1', 'h3_res_3', 'h3_res_4']\n",
      "Desired column order: ['lon', 'lat', 'time', 'Month', 'z_min', 'z_max', 'z_mean', 'r_min', 'r_max', 'r_mean', 't_min', 't_max', 't_mean', 'u_min', 'u_max', 'u_mean', 'v_min', 'v_max', 'v_mean', 'w_min', 'w_max', 'w_mean', 'vo_min', 'vo_max', 'vo_mean', 'DUCMASS_mean', 'DUFLUXU_min', 'DUFLUXU_max', 'DUFLUXU_mean', 'DUFLUXV_min', 'DUFLUXV_max', 'DUFLUXV_mean', 'DMI_EAST_HadISST1.1', 'DMI_HadISST1.1', 'lag_1', 'ensemble_predictions', 'squared_error_ensemble', 'lr_predictions', 'knn_predictions', 'dt_predictions', 'rf_predictions', 'h3_res_3', 'h3_res_4']\n",
      "Saved level averages to Z:\\Thesis\\Data\\test\\DustCast\\UA\\DC_v0063\\ua_level_averages_2023_v0063_H3res4_test.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "input_file = r\"Z:\\Thesis\\Data\\test\\DustCast\\UA\\DC_v0063\\ua_row_level_predictions_2023_v0063_H3res4.csv\"\n",
    "output_file = r\"Z:\\Thesis\\Data\\test\\DustCast\\UA\\DC_v0063\\ua_level_averages_2023_v0063_H3res4_test.csv\"\n",
    "\n",
    "# Desired levels to include\n",
    "levels_to_include = ['1000', '925', '850', '700', '500', '300', '200', '100', '50', '10']\n",
    "\n",
    "print(\"Loading CSV file...\")\n",
    "df = pd.read_csv(input_file)\n",
    "print(f\"Initial data shape: {df.shape}\")\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "# Convert \"level\" column to numeric then to integer string (so \"10.0\" becomes \"10\")\n",
    "if 'level' not in df.columns:\n",
    "    print(\"Error: 'level' column not found.\")\n",
    "else:\n",
    "    df['level'] = pd.to_numeric(df['level'], errors='coerce').astype('Int64').astype(str)\n",
    "print(\"Unique levels before filtering:\", df['level'].unique())\n",
    "\n",
    "# Filter rows to only include desired levels\n",
    "df = df[df['level'].isin(levels_to_include)]\n",
    "print(f\"Data shape after filtering levels: {df.shape}\")\n",
    "print(\"Unique levels after filtering:\", df['level'].unique())\n",
    "\n",
    "# Derive 'Month' from 'time' if not already present\n",
    "if 'Month' not in df.columns:\n",
    "    if 'time' in df.columns:\n",
    "        print(\"Deriving 'Month' from 'time' column...\")\n",
    "        df['time'] = pd.to_datetime(df['time'], errors='coerce')\n",
    "        df['Month'] = df['time'].dt.strftime('%Y-%m')\n",
    "    else:\n",
    "        print(\"Error: Neither 'Month' nor 'time' column found.\")\n",
    "print(\"Unique Months:\", df['Month'].unique())\n",
    "\n",
    "# Define grouping keys as (lon, lat, Month) so that we average over levels\n",
    "group_keys = ['lon', 'lat', 'Month']\n",
    "print(\"Grouping keys:\", group_keys)\n",
    "\n",
    "# List of columns for which to compute the average\n",
    "cols_to_avg = [\n",
    "    'z_min', 'z_max', 'z_mean',\n",
    "    'r_min', 'r_max', 'r_mean',\n",
    "    't_min', 't_max', 't_mean',\n",
    "    'u_min', 'u_max', 'u_mean',\n",
    "    'v_min', 'v_max', 'v_mean',\n",
    "    'w_min', 'w_max', 'w_mean',\n",
    "    'vo_min', 'vo_max', 'vo_mean',\n",
    "    'DUCMASS_mean',\n",
    "    'DUFLUXU_min', 'DUFLUXU_max', 'DUFLUXU_mean',\n",
    "    'DUFLUXV_min', 'DUFLUXV_max', 'DUFLUXV_mean',\n",
    "    'lag_1', 'ensemble_predictions', 'squared_error_ensemble',\n",
    "    'lr_predictions', 'knn_predictions', 'dt_predictions', 'rf_predictions'\n",
    "]\n",
    "\n",
    "# All other columns (that should remain the same) are those not in cols_to_avg and not in group_keys.\n",
    "cols_to_keep = [col for col in df.columns if col not in cols_to_avg and col not in group_keys]\n",
    "print(\"Columns to average:\", cols_to_avg)\n",
    "print(\"Other columns to keep:\", cols_to_keep)\n",
    "\n",
    "# Build aggregation dictionary: for numeric columns we compute the mean; for others, we take the first value.\n",
    "agg_dict = {col: 'mean' for col in cols_to_avg}\n",
    "agg_dict.update({col: 'first' for col in cols_to_keep})\n",
    "print(\"Aggregation dictionary:\")\n",
    "print(agg_dict)\n",
    "\n",
    "print(\"Grouping data by keys:\", group_keys)\n",
    "grouped = df.groupby(group_keys, as_index=False).agg(agg_dict)\n",
    "print(f\"Grouped data shape: {grouped.shape}\")\n",
    "print(\"Grouped data columns:\", grouped.columns.tolist())\n",
    "\n",
    "# Define desired column order. Note that \"level\" is omitted since we average across levels.\n",
    "desired_order = [\n",
    "    'lon', 'lat', 'time', 'Month',\n",
    "    'z_min', 'z_max', 'z_mean',\n",
    "    'r_min', 'r_max', 'r_mean',\n",
    "    't_min', 't_max', 't_mean',\n",
    "    'u_min', 'u_max', 'u_mean',\n",
    "    'v_min', 'v_max', 'v_mean',\n",
    "    'w_min', 'w_max', 'w_mean',\n",
    "    'vo_min', 'vo_max', 'vo_mean',\n",
    "    'DUCMASS_mean',\n",
    "    'DUFLUXU_min', 'DUFLUXU_max', 'DUFLUXU_mean',\n",
    "    'DUFLUXV_min', 'DUFLUXV_max', 'DUFLUXV_mean',\n",
    "    'DMI_EAST_HadISST1.1', 'DMI_HadISST1.1',  # if present\n",
    "    'lag_1', 'ensemble_predictions', 'squared_error_ensemble',\n",
    "    'lr_predictions', 'knn_predictions', 'dt_predictions', 'rf_predictions',\n",
    "    'h3_res_3', 'h3_res_4'\n",
    "]\n",
    "desired_order = [col for col in desired_order if col in grouped.columns]\n",
    "print(\"Desired column order:\", desired_order)\n",
    "grouped = grouped[desired_order]\n",
    "\n",
    "# Save the aggregated results to CSV\n",
    "grouped.to_csv(output_file, index=False)\n",
    "print(f\"Saved level averages to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1550cc8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
