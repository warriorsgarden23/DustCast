{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcb98e11-2589-49ef-beb9-938c2eeef9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paths defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: invalid escape sequence '\\T'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\T'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\T'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\T'\n",
      "C:\\Users\\Charl\\AppData\\Local\\Temp\\ipykernel_28156\\2103359288.py:7: SyntaxWarning: invalid escape sequence '\\T'\n",
      "  \"DMI_EAST_HadISST1.1\": \"Z:\\Thesis\\Data\\Met\\DMI\\dmieast.had.long.csv\",\n",
      "C:\\Users\\Charl\\AppData\\Local\\Temp\\ipykernel_28156\\2103359288.py:8: SyntaxWarning: invalid escape sequence '\\T'\n",
      "  \"DMI_HadISST1.1\": \"Z:\\Thesis\\Data\\Met\\DMI\\dmieast.had.long.csv\"\n",
      "C:\\Users\\Charl\\AppData\\Local\\Temp\\ipykernel_28156\\2103359288.py:7: SyntaxWarning: invalid escape sequence '\\T'\n",
      "  \"DMI_EAST_HadISST1.1\": \"Z:\\Thesis\\Data\\Met\\DMI\\dmieast.had.long.csv\",\n",
      "C:\\Users\\Charl\\AppData\\Local\\Temp\\ipykernel_28156\\2103359288.py:8: SyntaxWarning: invalid escape sequence '\\T'\n",
      "  \"DMI_HadISST1.1\": \"Z:\\Thesis\\Data\\Met\\DMI\\dmieast.had.long.csv\"\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['DMI_HadISST1.1'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m     combined_dmi_data \u001b[38;5;241m=\u001b[39m combined_dmi_data\u001b[38;5;241m.\u001b[39mloc[:, \u001b[38;5;241m~\u001b[39mcombined_dmi_data\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mduplicated()]\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m combined_dmi_data\n\u001b[1;32m---> 34\u001b[0m dmi_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_dmi_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdmi_csv_files\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_and_merge_parquet_files\u001b[39m(base_dir, years, dmi_data, output_dir):\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m year \u001b[38;5;129;01min\u001b[39;00m years:\n",
      "Cell \u001b[1;32mIn[8], line 27\u001b[0m, in \u001b[0;36mload_dmi_data\u001b[1;34m(file_paths)\u001b[0m\n\u001b[0;32m     25\u001b[0m     dmi_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m dmi_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m pd\u001b[38;5;241m.\u001b[39moffsets\u001b[38;5;241m.\u001b[39mMonthEnd(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# Rename the value column to match the key for clarity\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m     dmi_data \u001b[38;5;241m=\u001b[39m \u001b[43mdmi_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     28\u001b[0m     dmi_frames\u001b[38;5;241m.\u001b[39mappend(dmi_data)\n\u001b[0;32m     29\u001b[0m combined_dmi_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(dmi_frames, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ERA5\\Lib\\site-packages\\pandas\\core\\frame.py:4096\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4094\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4095\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4096\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4098\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ERA5\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ERA5\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['DMI_HadISST1.1'] not in index\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Paths to DMI CSV files\n",
    "dmi_csv_files = {\n",
    "    \"DMI_EAST_HadISST1.1\": r\"Z:\\Thesis\\Data\\Met\\DMI\\dmieast.had.long.csv\",\n",
    "    \"DMI_HadISST1.1\": r\"Z:\\Thesis\\Data\\Met\\DMI\\dmieast.had.long.csv\"\n",
    "}\n",
    "\n",
    "# Paths to parquet files\n",
    "data_directory = \"Z:\\\\Thesis\\\\Data\\\\ML_Data\\\\AP__ML_training_data\"\n",
    "output_directory = \"Z:\\\\Thesis\\\\Data\\\\ML_Data\\\\AP__ML_training_data\"\n",
    "years = list(range(1980, 1982)) #+ list(range(2016, 2024))\n",
    "\n",
    "print(f\"paths defined\")\n",
    "\n",
    "# Read and preprocess DMI data\n",
    "def load_dmi_data(file_paths):\n",
    "    dmi_frames = []\n",
    "    for column_name, file_path in file_paths.items():\n",
    "        dmi_data = pd.read_csv(file_path)\n",
    "        # Parse dates and set them to the last day of the month\n",
    "        dmi_data['Date'] = pd.to_datetime(dmi_data['Date'], format='%m/%d/%Y', errors='coerce')\n",
    "        dmi_data['Date'] = dmi_data['Date'] + pd.offsets.MonthEnd(0)\n",
    "        # Rename the value column to match the key for clarity\n",
    "        dmi_data = dmi_data[['Date', column_name]]\n",
    "        dmi_frames.append(dmi_data)\n",
    "    combined_dmi_data = pd.concat(dmi_frames, axis=1)\n",
    "    # Remove duplicate 'Date' columns, keeping only one\n",
    "    combined_dmi_data = combined_dmi_data.loc[:, ~combined_dmi_data.columns.duplicated()]\n",
    "    return combined_dmi_data\n",
    "\n",
    "dmi_data = load_dmi_data(dmi_csv_files)\n",
    "\n",
    "def process_and_merge_parquet_files(base_dir, years, dmi_data, output_dir):\n",
    "    for year in years:\n",
    "        year_path = os.path.join(base_dir, str(year))\n",
    "        parquet_files = glob(os.path.join(year_path, \"*.parquet\"))\n",
    "\n",
    "        for parquet_file in parquet_files:\n",
    "            # Load parquet file\n",
    "            parquet_data = pd.read_parquet(parquet_file)\n",
    "\n",
    "            # Ensure the 'time' column is in datetime format and set to the last day of the month\n",
    "            parquet_data['time'] = pd.to_datetime(parquet_data['time'], errors='coerce')\n",
    "            parquet_data['time'] = parquet_data['time'] + pd.offsets.MonthEnd(0)\n",
    "\n",
    "            # Merge with DMI data\n",
    "            merged_data = parquet_data.merge(\n",
    "                dmi_data,\n",
    "                left_on='time',\n",
    "                right_on='Date',\n",
    "                how='left'\n",
    "            )\n",
    "\n",
    "            # Drop unnecessary 'Date' column from the output\n",
    "            merged_data = merged_data.drop(columns=['Date'])\n",
    "\n",
    "            # Save merged data to a new parquet file in the output directory\n",
    "            relative_path = os.path.relpath(parquet_file, base_dir)\n",
    "            output_file = os.path.join(output_dir, relative_path)\n",
    "            os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "            merged_data.to_parquet(output_file, index=False)\n",
    "\n",
    "            print(f\"Merged data saved to {output_file}\")\n",
    "\n",
    "# Run the merging process\n",
    "process_and_merge_parquet_files(data_directory, years, dmi_data, output_directory)\n",
    "\n",
    "print(f\"process completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a35e30a-ad36-4b09-8f42-ddc932319302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths defined\n",
      "DMI data loaded and processed:\n",
      "     year  month  DMI_EAST_HadISST1.1  DMI_HadISST1.1\n",
      "0  1900.0    1.0               -0.226          -0.403\n",
      "1  1900.0    2.0               -0.105          -0.213\n",
      "2  1900.0    3.0               -0.163          -0.364\n",
      "3  1900.0    4.0               -0.097          -0.244\n",
      "4  1900.0    5.0               -0.304          -0.080\n",
      "\n",
      "Processing directory: Z:\\Thesis\\Data\\ML_Data\\AP_ML_training_data\\1980\n",
      "Found 14 parquet files.\n",
      "Merged data saved to Z:\\Thesis\\Data\\ML_Data\\AP_ML_training_data\\1980\\Bahrain_1980_pressure_monthly_stats_merged.parquet\n",
      "Merged data saved to Z:\\Thesis\\Data\\ML_Data\\AP_ML_training_data\\1980\\Bahrain_1980_surface_monthly_stats_merged.parquet\n",
      "Merged data saved to Z:\\Thesis\\Data\\ML_Data\\AP_ML_training_data\\1980\\Kuwait_1980_pressure_monthly_stats_merged.parquet\n",
      "Merged data saved to Z:\\Thesis\\Data\\ML_Data\\AP_ML_training_data\\1980\\Kuwait_1980_surface_monthly_stats_merged.parquet\n",
      "Merged data saved to Z:\\Thesis\\Data\\ML_Data\\AP_ML_training_data\\1980\\Oman_1980_pressure_monthly_stats_merged.parquet\n",
      "Merged data saved to Z:\\Thesis\\Data\\ML_Data\\AP_ML_training_data\\1980\\Oman_1980_surface_monthly_stats_merged.parquet\n",
      "Merged data saved to Z:\\Thesis\\Data\\ML_Data\\AP_ML_training_data\\1980\\Qatar_1980_pressure_monthly_stats_merged.parquet\n",
      "Merged data saved to Z:\\Thesis\\Data\\ML_Data\\AP_ML_training_data\\1980\\Qatar_1980_surface_monthly_stats_merged.parquet\n",
      "Merged data saved to Z:\\Thesis\\Data\\ML_Data\\AP_ML_training_data\\1980\\Saudi_Arabia_1980_pressure_monthly_stats_merged.parquet\n",
      "Merged data saved to Z:\\Thesis\\Data\\ML_Data\\AP_ML_training_data\\1980\\Saudi_Arabia_1980_surface_monthly_stats_merged.parquet\n",
      "Merged data saved to Z:\\Thesis\\Data\\ML_Data\\AP_ML_training_data\\1980\\United_Arab_Emirates_1980_pressure_monthly_stats_merged.parquet\n",
      "Merged data saved to Z:\\Thesis\\Data\\ML_Data\\AP_ML_training_data\\1980\\United_Arab_Emirates_1980_surface_monthly_stats_merged.parquet\n",
      "Merged data saved to Z:\\Thesis\\Data\\ML_Data\\AP_ML_training_data\\1980\\Yemen_1980_pressure_monthly_stats_merged.parquet\n",
      "Merged data saved to Z:\\Thesis\\Data\\ML_Data\\AP_ML_training_data\\1980\\Yemen_1980_surface_monthly_stats_merged.parquet\n",
      "\n",
      "Processing directory: Z:\\Thesis\\Data\\ML_Data\\AP_ML_training_data\\1981\n",
      "Found 14 parquet files.\n",
      "Merged data saved to Z:\\Thesis\\Data\\ML_Data\\AP_ML_training_data\\1981\\Bahrain_1981_pressure_monthly_stats_merged.parquet\n",
      "Merged data saved to Z:\\Thesis\\Data\\ML_Data\\AP_ML_training_data\\1981\\Bahrain_1981_surface_monthly_stats_merged.parquet\n",
      "Merged data saved to Z:\\Thesis\\Data\\ML_Data\\AP_ML_training_data\\1981\\Kuwait_1981_pressure_monthly_stats_merged.parquet\n",
      "Merged data saved to Z:\\Thesis\\Data\\ML_Data\\AP_ML_training_data\\1981\\Kuwait_1981_surface_monthly_stats_merged.parquet\n",
      "Merged data saved to Z:\\Thesis\\Data\\ML_Data\\AP_ML_training_data\\1981\\Oman_1981_pressure_monthly_stats_merged.parquet\n",
      "Merged data saved to Z:\\Thesis\\Data\\ML_Data\\AP_ML_training_data\\1981\\Oman_1981_surface_monthly_stats_merged.parquet\n",
      "Merged data saved to Z:\\Thesis\\Data\\ML_Data\\AP_ML_training_data\\1981\\Qatar_1981_pressure_monthly_stats_merged.parquet\n",
      "Merged data saved to Z:\\Thesis\\Data\\ML_Data\\AP_ML_training_data\\1981\\Qatar_1981_surface_monthly_stats_merged.parquet\n",
      "Merged data saved to Z:\\Thesis\\Data\\ML_Data\\AP_ML_training_data\\1981\\Saudi_Arabia_1981_pressure_monthly_stats_merged.parquet\n",
      "Merged data saved to Z:\\Thesis\\Data\\ML_Data\\AP_ML_training_data\\1981\\Saudi_Arabia_1981_surface_monthly_stats_merged.parquet\n",
      "Merged data saved to Z:\\Thesis\\Data\\ML_Data\\AP_ML_training_data\\1981\\United_Arab_Emirates_1981_pressure_monthly_stats_merged.parquet\n",
      "Merged data saved to Z:\\Thesis\\Data\\ML_Data\\AP_ML_training_data\\1981\\United_Arab_Emirates_1981_surface_monthly_stats_merged.parquet\n",
      "Merged data saved to Z:\\Thesis\\Data\\ML_Data\\AP_ML_training_data\\1981\\Yemen_1981_pressure_monthly_stats_merged.parquet\n",
      "Merged data saved to Z:\\Thesis\\Data\\ML_Data\\AP_ML_training_data\\1981\\Yemen_1981_surface_monthly_stats_merged.parquet\n",
      "\n",
      "Process completed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Paths to DMI CSV files (using raw strings to avoid escape issues)\n",
    "dmi_csv_files = {\n",
    "    \"DMI_EAST_HadISST1.1\": r\"Z:\\Thesis\\Data\\Met\\DMI\\dmieast.had.long.csv\",\n",
    "    \"DMI_HadISST1.1\": r\"Z:\\Thesis\\Data\\Met\\DMI\\dmi.had.long.csv\"\n",
    "}\n",
    "\n",
    "# Paths to parquet files\n",
    "data_directory = r\"Z:\\Thesis\\Data\\ML_Data\\AP_ML_training_data\"\n",
    "output_directory = r\"Z:\\Thesis\\Data\\ML_Data\\AP_ML_training_data\"\n",
    "years = list(range(1980, 2000))  # Adjust the range of years as needed\n",
    "\n",
    "print(\"Paths defined\")\n",
    "\n",
    "def load_dmi_data(file_paths):\n",
    "    \"\"\"\n",
    "    Loads DMI CSV files, converts the date string to a datetime,\n",
    "    creates 'year' and 'month' columns, and then merges the datasets\n",
    "    on these two columns.\n",
    "    \"\"\"\n",
    "    dmi_data_final = None\n",
    "    for column_name, file_path in file_paths.items():\n",
    "        # Use comma as the delimiter\n",
    "        dmi_data = pd.read_csv(file_path, sep=',')\n",
    "        # Remove any accidental spaces in the column names\n",
    "        dmi_data.columns = dmi_data.columns.str.strip()\n",
    "        # Convert the Date column (e.g. \"1/1/1980\") to datetime\n",
    "        dmi_data['Date'] = pd.to_datetime(dmi_data['Date'], format='%m/%d/%Y', errors='coerce')\n",
    "        # Create separate year and month columns\n",
    "        dmi_data['year'] = dmi_data['Date'].dt.year\n",
    "        dmi_data['month'] = dmi_data['Date'].dt.month\n",
    "        # Keep only the year, month, and the desired DMI column\n",
    "        dmi_data = dmi_data[['year', 'month', column_name]]\n",
    "        \n",
    "        # Merge this dataframe with the accumulated data\n",
    "        if dmi_data_final is None:\n",
    "            dmi_data_final = dmi_data\n",
    "        else:\n",
    "            dmi_data_final = pd.merge(dmi_data_final, dmi_data, on=['year', 'month'], how='outer')\n",
    "            \n",
    "    return dmi_data_final\n",
    "\n",
    "dmi_data = load_dmi_data(dmi_csv_files)\n",
    "print(\"DMI data loaded and processed:\")\n",
    "print(dmi_data.head())\n",
    "\n",
    "def process_and_merge_parquet_files(base_dir, years, dmi_data, output_dir):\n",
    "    \"\"\"\n",
    "    For each parquet file in each year's directory:\n",
    "      - Drop the existing (blank) DMI columns from the parquet data,\n",
    "      - Convert the 'time' column to datetime and create 'year' and 'month' columns,\n",
    "      - Merge with the DMI data on ['year', 'month'],\n",
    "      - Save the merged dataset.\n",
    "    \"\"\"\n",
    "    for year in years:\n",
    "        year_path = os.path.join(base_dir, str(year))\n",
    "        print(f\"\\nProcessing directory: {year_path}\")\n",
    "        parquet_files = glob(os.path.join(year_path, \"*.parquet\"))\n",
    "        print(f\"Found {len(parquet_files)} parquet files.\")\n",
    "        \n",
    "        if not parquet_files:\n",
    "            print(f\"No parquet files found for {year}\")\n",
    "            continue\n",
    "\n",
    "        for parquet_file in parquet_files:\n",
    "            try:\n",
    "                parquet_data = pd.read_parquet(parquet_file)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {parquet_file}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Drop the blank DMI columns from the parquet file (if they exist)\n",
    "            for col in [\"DMI_EAST_HadISST1.1\", \"DMI_HadISST1.1\"]:\n",
    "                if col in parquet_data.columns:\n",
    "                    parquet_data.drop(columns=[col], inplace=True)\n",
    "\n",
    "            # Convert the 'time' column to datetime (format like \"1/31/1980\")\n",
    "            parquet_data['time'] = pd.to_datetime(parquet_data['time'], format='%m/%d/%Y', errors='coerce')\n",
    "            # Create year and month columns for merging\n",
    "            parquet_data['year'] = parquet_data['time'].dt.year\n",
    "            parquet_data['month'] = parquet_data['time'].dt.month\n",
    "\n",
    "            # Merge on year and month\n",
    "            merged_data = parquet_data.merge(dmi_data, on=['year', 'month'], how='left')\n",
    "\n",
    "            # Build the output file path and ensure the directory exists\n",
    "            relative_path = os.path.relpath(parquet_file, base_dir)\n",
    "            output_file = os.path.join(output_dir, relative_path)\n",
    "            os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "            merged_data.to_parquet(output_file, index=False)\n",
    "\n",
    "            print(f\"Merged data saved to {output_file}\")\n",
    "\n",
    "process_and_merge_parquet_files(data_directory, years, dmi_data, output_directory)\n",
    "print(\"\\nProcess completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba4402c-206b-418f-918e-1e2a0357b5f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
